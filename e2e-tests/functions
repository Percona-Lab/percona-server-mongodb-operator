#!/bin/bash

GIT_COMMIT=$(git rev-parse HEAD)
GIT_BRANCH=${VERSION:-$(git rev-parse --abbrev-ref HEAD | sed -e 's^/^-^g; s^[.]^-^g;' | tr '[:upper:]' '[:lower:]')}
IMAGE=${IMAGE:-"perconalab/percona-server-mongodb-operator:${GIT_BRANCH}"}
IMAGE_MONGOD36=${IMAGE_MONGOD36:-"perconalab/percona-server-mongodb-operator:${GIT_BRANCH}-mongod3.6"}
tmp_dir=$(mktemp -d)
sed=$(which gsed || which sed)
date=$(which gdate || which date)

test_name=$(basename $test_dir)
namespace="${test_name}-${RANDOM}"
conf_dir=$(realpath $test_dir/../conf || :)
src_dir=$(realpath $test_dir/../..)
if oc version | grep -q openshift; then
    OPENSHIFT=1
fi


create_namespace() {
    local namespace="$1"
    if [ "$OPENSHIFT" == 1 ]; then
        oc delete project "$namespace" && sleep 40 || :
        oc new-project "$namespace"
        oc project "$namespace"
        oc adm policy add-scc-to-user hostaccess -z default || :
    else
        kubectl delete namespace "$namespace" || :
        wait_for_delete "namespace/$namespace"
        kubectl create namespace "$namespace"
        kubectl config set-context $(kubectl config current-context) --namespace="$namespace"
    fi
}

get_operator_pod() {
    kubectl get pods \
        --selector=name=percona-server-mongodb-operator \
        -o 'jsonpath={.items[].metadata.name}'
}

wait_pod() {
    local pod=$1

    set +o xtrace
    retry=0
    echo -n $pod
    #until kubectl get pod/$pod -o jsonpath='{.status.phase}' 2>/dev/null | grep 'Running'; do
    until kubectl get pod/$pod -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null | grep 'true'; do
        sleep 1
        echo -n .
        let retry+=1
        if [ $retry -ge 60 ]; then
            kubectl describe pod/$pod
            kubectl logs $pod
            kubectl logs $(get_operator_pod) \
                | grep -v 'level=info' \
                | grep -v 'level=debug' \
                | grep -v 'Getting tasks for pod' \
                | grep -v 'Getting pods from source'
            echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
            exit 1
        fi
    done
    set -o xtrace
}

wait_cron() {
    local backup=$1

    set +o xtrace
    retry=0
    echo -n $backup
    until kubectl get cronjob/$backup -o jsonpath='{.status.lastScheduleTime}' 2>/dev/null | grep 'T'; do
        sleep 1
        echo -n .
        let retry+=1
        if [ $retry -ge 360 ]; then
            kubectl logs $(get_operator_pod) \
                | grep -v 'level=info' \
                | grep -v 'level=debug' \
                | grep -v 'Getting tasks for pod' \
                | grep -v 'Getting pods from source'
            echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
            exit 1
        fi
    done
    set -o xtrace
}

wait_backup() {
    local backup=$1

    set +o xtrace
    retry=0
    echo -n $backup
    until kubectl get job.batch/$backup -o jsonpath='{.status.completionTime}' 2>/dev/null | grep 'T'; do
        sleep 1
        echo -n .
        let retry+=1
        if [ $retry -ge 60 ]; then
            kubectl logs $(get_operator_pod) \
                | grep -v 'level=info' \
                | grep -v 'level=debug' \
                | grep -v 'Getting tasks for pod' \
                | grep -v 'Getting pods from source'
            echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
            exit 1
        fi
    done
    set -o xtrace
}

deploy_operator() {
    desc 'start operator'
    sed -e "s^image: .*^image: ${IMAGE}^" \
        ${src_dir}/deploy/operator.yaml \
        > ${tmp_dir}/operator.yml

    kubectl apply -f ${src_dir}/deploy/crd.yaml || :
    kubectl apply \
        -f ${src_dir}/deploy/rbac.yaml \
        -f ${tmp_dir}/operator.yml
    sleep 2

    wait_pod $(get_operator_pod)
}

deploy_helm() {
    local namespace="$1"
    if [ "$OPENSHIFT" == 1 ]; then
        export TILLER_NAMESPACE=tiller
        oc new-project tiller || :
        oc project tiller
        oc process -f https://github.com/openshift/origin/raw/master/examples/helm/tiller-template.yaml -p TILLER_NAMESPACE="tiller" -p HELM_VERSION=v2.12.1 | oc apply -f -
    else
        kubectl --namespace kube-system create sa tiller || :
        kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller || :
        helm init --service-account tiller
        kubectl config set-context $(kubectl config current-context) --namespace="kube-system"
    fi

    tiller_pod=$(
        kubectl get pods \
            --selector=name=tiller \
            -o 'jsonpath={.items[].metadata.name}'
    )
    wait_pod $tiller_pod

    if [ "$OPENSHIFT" == 1 ]; then
        oc project "$namespace"
        oc policy add-role-to-user edit "system:serviceaccount:tiller:tiller"
    else
        kubectl config set-context $(kubectl config current-context) --namespace="$namespace"
    fi
}

wait_for_running() {
    local name="$1"
    let last_pod="$2-1" || :

    for i in $(seq 0 $last_pod); do
        wait_pod ${name}-${i}
    done
}

wait_for_delete() {
    local res="$1"

    set +o xtrace
    echo -n "$res - "
    retry=0
    until (kubectl get $res || :) 2>&1 | grep NotFound; do
        sleep 1
        echo -n .
        let retry+=1
        if [ $retry -ge 60 ]; then
            kubectl logs $(get_operator_pod) \
                | grep -v 'level=info' \
                | grep -v 'level=debug' \
                | grep -v 'Getting tasks for pod' \
                | grep -v 'Getting pods from source'
            echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
            exit 1
        fi
    done
    set -o xtrace
}

compare_kubectl() {
    local resource="$1"
    local postfix="$2"
    local expected_result=${test_dir}/compare/${resource//\//_}${postfix}.yml
    local new_result="${tmp_dir}/${resource//\//_}.yml"

    if [ "$OPENSHIFT" = 1 -a -f ${expected_result//.yml/-oc.yml} ]; then
        local expected_result=${expected_result//.yml/-oc.yml}
    fi

    kubectl get -o yaml ${resource} \
        | egrep -v "namespace:|uid:|resourceVersion:|selfLink:|creationTimestamp:|image:|clusterIP:" \
        | egrep -v "^  storageClassName:|finalizers:|kubernetes.io/pvc-protection|volumeName:|storage-provisioner:" \
        | egrep -v "healthCheckNodePort:|nodePort:" \
        | $sed -e '/^status:$/,+100500d' \
        | $sed -e '/NAMESPACE/,+1d' \
        | $sed -e '/PBM_AGENT_SERVER_ADDRESS/,+1d' \
        > ${new_result}
    diff -u ${expected_result} ${new_result}
}

run_mongo() {
    local command="$1"
    local uri="$2"
    local driver=${3:-mongodb+srv}
    local suffix=${4:-.svc.cluster.local}
    local client_container=$(kubectl get pods --selector=name=psmdb-client -o 'jsonpath={.items[].metadata.name}')

    kubectl exec ${client_container} -- \
        bash -c "printf '$command\n' | mongo $driver://$uri$suffix/admin?ssl=false\&replicaSet=rs0"
}

get_service_ip() {
    local service=$1
    if [ "$(kubectl get psmdb/${service/-rs0*/} -o 'jsonpath={.spec.replsets[].expose.enabled}')" != "true" ]; then
        echo -n $service.${service/-rs0*/}-rs0.$namespace
        return
    fi
    while (kubectl get service/$service -o 'jsonpath={.spec.type}' 2>&1 || : ) | grep -q NotFound; do
        sleep 1
    done
    if [ "$(kubectl get service/$service -o 'jsonpath={.spec.type}')" = "ClusterIP" ]; then
        kubectl get service/$service -o 'jsonpath={.spec.clusterIP}'
        return
    fi
    until kubectl get service/$service -o 'jsonpath={.status.loadBalancer.ingress[]}' 2>&1 | egrep -q "hostname|ip"; do
        sleep 1
    done
    kubectl get service/$service -o 'jsonpath={.status.loadBalancer.ingress[].ip}'
    kubectl get service/$service -o 'jsonpath={.status.loadBalancer.ingress[].hostname}'
}

compare_mongo_cmd() {
    local command="$1"
    local uri="$2"
    local postfix="$3"
    local suffix="$4"

    run_mongo "use myApp\n db.test.${command}()" "$uri" "mongodb" "$suffix" \
        | egrep -v 'I NETWORK|W NETWORK|Error saving history file|Percona Server for MongoDB|connecting to:|Unable to reach primary for set|Implicit session:' \
        | $sed -re 's/ObjectId\("[0-9a-f]+"\)//; s/-[0-9]+.svc/-xxx.svc/' \
        > $tmp_dir/${command}
    diff ${test_dir}/compare/${command}${postfix}.json $tmp_dir/${command}
}

get_mongo_primary_endpoint() {
    local uri="$1"

    run_mongo 'db.isMaster().me' "$uri" "mongodb" ":27017" \
        | egrep -v "Time|Percona Server for MongoDB|bye|BinData|NumberLong|connecting to|Error saving history file|I NETWORK|W NETWORK|Implicit session:" \
        | sed -e 's^20[0-9][0-9]-[0-9][0-9]-[0-9][0-9]T[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\.[0-9][0-9][0-9]+[0-9][0-9][0-9][0-9]^^' \
        | grep ":27017$"
}

get_mongo_primary() {
    local uri="$1"

    endpoint=$(get_mongo_primary_endpoint $uri)
    if [[ "$endpoint" =~ ".local" ]]; then
        echo $endpoint \
            | cut -d . -f 1
    else
        kubectl get service -o wide \
            | grep " ${endpoint/:*/} " \
            | awk '{print$1}'
    fi
}

compare_mongo_user() {
    local uri="$1"
    local user=$(echo $uri | cut -d : -f 1)

    run_mongo 'db.runCommand({connectionStatus:1,showPrivileges:true})' "$uri" \
        | egrep -v "Time|Percona Server for MongoDB|bye|BinData|NumberLong|connecting to|Error saving history file|I NETWORK|W NETWORK|Implicit session:" \
        | sed -e 's^20[0-9][0-9]-[0-9][0-9]-[0-9][0-9]T[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\.[0-9][0-9][0-9]+[0-9][0-9][0-9][0-9]^^' \
        | $sed -e '/"ok" : 1/,+4d' \
        > $tmp_dir/$user.json
    diff ${test_dir}/compare/$user.json $tmp_dir/$user.json
}

start_gke() {
    gcloud container clusters create operator-testing-$RANDOM --zone europe-west3-c --project cloud-dev-112233 --preemptible --cluster-version 1.11
}

get_pumba() {
    kubectl get pods \
        --selector=name=pumba \
        -o 'jsonpath={.items[].metadata.name}'
}

run_pumba() {
    local cmd="$*"
    kubectl exec -it "$(get_pumba)" -- /pumba -l info ${cmd}
}

destroy() {
    local namespace="$1"

    kubectl logs $(get_operator_pod) \
        | grep -v 'level=info' \
        | grep -v 'level=debug' \
        | grep -v 'Getting tasks for pod' \
        | grep -v 'Getting pods from source' \
        | grep -v 'the object has been modified' \
        | grep -v 'get backup status: Job.batch' \
        | $sed -r 's/"ts":[0-9.]+//; s^limits-[0-9.]+/^^g' \
        | sort -u \
        | tee $tmp_dir/operator.log

    #TODO: maybe will be enabled later
    #diff $test_dir/compare/operator.log $tmp_dir/operator.log

    if [ "$OPENSHIFT" == 1 ]; then
        oc delete project "$namespace"
    else
        kubectl delete namespace "$namespace"
    fi
    rm -rf ${tmp_dir}
}

desc() {
    set +o xtrace
    local msg="$@"
    printf "\n\n-----------------------------------------------------------------------------------\n"
    printf "$msg"
    printf "\n-----------------------------------------------------------------------------------\n\n"
    set -o xtrace
}

get_backup_name() {
    local storage_name="$1"
    local pod_name="$2"
    let last_pod="$3-1" || :

    for i in $(seq 0 $last_pod); do
        kubectl logs ${pod_name}-${i} -c backup-agent \
            | grep db_backup_name \
            | grep $storage_name \
            | $sed -re 's/.*db_backup_name:["\]+([^"\]+).*/\1/'
    done
}
